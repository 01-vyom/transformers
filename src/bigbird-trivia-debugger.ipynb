{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import official.nlp.projects.triviaqa.modeling as modeling\n",
    "from official.nlp.configs.encoders import EncoderConfig\n",
    "import os\n",
    "\n",
    "from transformers import BigBirdConfig, BigBirdModel, BigBirdForQuestionAnswering\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def difference_between_tensors(tf_tensor, pt_tensor):\n",
    "    tf_np = np.array(tf_tensor)\n",
    "    pt_np = np.array(pt_tensor.detach())\n",
    "    return np.max(np.abs(tf_np - pt_np))\n",
    "\n",
    "model_id = \"bigbird-base-trivia-itc\"\n",
    "TF_CKPT_DIR = f\"ckpt/{model_id}/model.ckpt-0\"\n",
    "HF_CKPT_DIR = f\"google/{model_id}/pytorch_model.bin\""
   ]
  },
  {
   "source": [
    "seqlen = 4096\n",
    "config = EncoderConfig(type=\"bigbird\")\n",
    "# block_size=16, num_rand_blocks=3\n",
    "hf_config = BigBirdConfig(num_hidden_layers=config.bigbird.num_layers, hidden_act=\"gelu_fast\", attention_type=\"block_sparse\", num_random_blocks=config.bigbird.num_rand_blocks, **config.bigbird.__dict__)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(TensorShape([1, 4096]), TensorShape([1]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "arr = np.random.randint(1, seqlen, size=seqlen).reshape(1, seqlen)\n",
    "\n",
    "input_ids = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
    "hf_input_ids = torch.from_numpy(arr).long()\n",
    "input_ids.shape, tf.constant([8]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:Encoder class: BigBirdEncoder to build...\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fed2fc9cd30>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_1', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fed2fc9cbb0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_2', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fecf0f69be0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_3', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fed2fce38b0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_4', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fecf0fa2460>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_5', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fecf105ad30>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_6', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fecf0457a90>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_7', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fecf103c3d0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_8', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fecf0ef8670>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_9', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fecf12ab910>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_10', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fecf878fbe0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_11', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fecfc1f8e80>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fedf1b47790>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "100%|██████████| 205/205 [00:00<00:00, 468.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# loading tf weights\n",
    "savedmodel = tf.saved_model.load(os.path.join(\"ckpt\", model_id))\n",
    "model = modeling.TriviaQaModel(config, seqlen)\n",
    "_ = model(dict(\n",
    "    token_ids=input_ids,\n",
    "    question_lengths=tf.constant([8], dtype=tf.int32,)\n",
    "))\n",
    "model.set_weights([v.numpy() for v in tqdm(savedmodel.trainable_variables)])\n",
    "del savedmodel\n",
    "\n",
    "# loading hf weights\n",
    "hf_model = BigBirdForQuestionAnswering.from_pretrained(f\"google/{model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(dict(\n",
    "    token_ids=input_ids,\n",
    "    question_lengths=tf.constant([8], dtype=tf.int32,)\n",
    "))\n",
    "start_logits, end_logits = out[:,:,0], out[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_start_logits, hf_end_logits = hf_model(hf_input_ids).to_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4096), dtype=float32, numpy=\n",
       " array([[-9.99999625e+05, -1.00000994e+06, -1.00000800e+06, ...,\n",
       "         -1.31911230e+01,  3.67996097e-01, -9.05163383e+00]], dtype=float32)>,\n",
       " tensor([[ 1.2715, -1.1993, -0.1114,  ...,  1.2939,  1.1098,  1.3245]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "start_logits, hf_start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4096), dtype=float32, numpy=\n",
       " array([[-1.00000200e+06, -1.00001206e+06, -1.00001181e+06, ...,\n",
       "         -1.49152393e+01, -2.01539540e+00, -1.23520317e+01]], dtype=float32)>,\n",
       " tensor([[ 1.5141,  0.2146, -0.2144,  ...,  1.5398,  1.3746,  1.5566]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "end_logits, hf_end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = modeling.TriviaQaModel(config, seqlen)\n",
    "\n",
    "# # building all the weights before setting-up :)\n",
    "# sequence_output, pooler_output = model(input_ids, training=False)\n",
    "\n",
    "# hf_model = BigBirdForPreTraining(hf_bigbird_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"difference bw input_ids:\", difference_between_tensors(model.input_ids, hf_model.bert.input_ids))\n",
    "# print(\"difference bw word_embeddings:\", difference_between_tensors(model.word_embeddings, hf_model.bert.word_embeddings))\n",
    "\n",
    "# print(\"difference bw l1 layer_input\", difference_between_tensors(model.encoder.l1_layer_input, hf_model.bert.encoder.l1_layer_input))\n",
    "\n",
    "# print(\"difference bw l1 layer_output\", difference_between_tensors(model.encoder.l1_layer_output, hf_model.bert.encoder.l1_layer_output))\n",
    "# print(\"difference bw last layer_output\", difference_between_tensors(model.encoder.last_layer_output,hf_model.bert.encoder.last_layer_output))\n",
    "\n",
    "# print(\"difference bw bigbird sequence out\", difference_between_tensors(sequence_output, hf_sequence_output), end=\"\\n\\n\")\n",
    "\n",
    "# print(\"difference bw bigbird pooler output\", difference_between_tensors(pooler_output, hf_pooler_output), end=\"\\n\\n\")\n",
    "\n",
    "# print(\"difference bw bigbird masked_lm_log_probs\", difference_between_tensors(masked_lm_log_probs, hf_masked_lm_log_probs), end=\"\\n\\n\")\n",
    "# print(\"difference bw bigbird next_sentence_log_probs\", difference_between_tensors(next_sentence_log_probs, hf_next_sentence_log_probs), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('transformers': conda)",
   "metadata": {
    "interpreter": {
     "hash": "43b8762cdba22b1f3661f53828ffc27c829b0d988d5ae49721f2db103a874ee7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}