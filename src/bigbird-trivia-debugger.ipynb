{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import official.nlp.projects.triviaqa.modeling as modeling\n",
    "from official.nlp.configs.encoders import EncoderConfig, build_encoder\n",
    "import os\n",
    "\n",
    "from transformers import BigBirdConfig, BigBirdModel, BigBirdForQuestionAnswering\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def difference_between_tensors(tf_tensor, pt_tensor):\n",
    "    tf_np = np.array(tf_tensor)\n",
    "    pt_np = np.array(pt_tensor.detach())\n",
    "    return np.max(np.abs(tf_np - pt_np))\n",
    "\n",
    "model_id = \"bigbird-base-trivia-itc\"\n",
    "TF_CKPT_DIR = f\"ckpt/{model_id}/model.ckpt-0\"\n",
    "HF_CKPT_DIR = f\"google/{model_id}/pytorch_model.bin\""
   ]
  },
  {
   "source": [
    "seqlen = 1024 # min seqlen we can keep in this case\n",
    "config = EncoderConfig(type=\"bigbird\")\n",
    "config.bigbird.block_size = 16\n",
    "hf_config = BigBirdConfig(num_hidden_layers=config.bigbird.num_layers, hidden_act=\"gelu_fast\", attention_type=\"block_sparse\", num_random_blocks=config.bigbird.num_rand_blocks, **config.bigbird.__dict__)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "arr = np.random.randint(1, seqlen, size=seqlen).reshape(1, seqlen)\n",
    "sep_pos = 9\n",
    "arr[:, sep_pos] = 66 # sep_id\n",
    "\n",
    "input_ids = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
    "question_lengths = tf.constant([sep_pos+1], dtype=tf.int32)\n",
    "\n",
    "hf_input_ids = torch.from_numpy(arr).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:Encoder class: BigBirdEncoder to build...\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc71a61a430>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_1', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6dbd32eb0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_2', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6dbd1f280>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_3', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6dc7f6d00>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_4', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6dbc9d700>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_5', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc71a5dcb50>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_6', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6dca6b1c0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_7', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6dc7805b0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_8', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6dca069a0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_9', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6e8d27d90>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_10', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6ead591f0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_11', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7fc6edc405e0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7fc7dca2c4c0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "100%|██████████| 205/205 [00:00<00:00, 314.78it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model weights loaded'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# loading tf weights\n",
    "savedmodel = tf.saved_model.load(os.path.join(\"ckpt\", model_id))\n",
    "model = modeling.TriviaQaModel(config, seqlen)\n",
    "#encoder = build_encoder(config)\n",
    "# qa_head = modeling.TriviaQaHead(\n",
    "#         config.get().intermediate_size,\n",
    "#         dropout_rate=config.get().dropout_rate,\n",
    "#         attention_dropout_rate=config.get().attention_dropout_rate)\n",
    "\n",
    "_ = model(dict(\n",
    "    token_ids=input_ids,\n",
    "    question_lengths=question_lengths\n",
    "))\n",
    "\n",
    "# x = model.encoder(dict(\n",
    "#     input_word_ids=inputs['token_ids'],\n",
    "#     input_mask=tf.cast(inputs['token_ids'] > 0, tf.int32),\n",
    "#     input_type_ids=1 - tf.sequence_mask(inputs['question_lengths'], seqlen, tf.int32)\n",
    "# ))\n",
    "\n",
    "# _ = model.qa_head(dict(\n",
    "#     token_embeddings=x['sequence_output'], \n",
    "#     token_ids=inputs['token_ids'],\n",
    "#     question_lengths=inputs['question_lengths']\n",
    "# ))\n",
    "\n",
    "# enc_vars = [v.name for v in encoder.variables]\n",
    "# qa_vars = [v.name for v in qa_head.variables]\n",
    "\n",
    "# encoder.set_weights([v.numpy() for v in tqdm(savedmodel.variables) if v.name in enc_vars])\n",
    "# qa_head.set_weights([v.numpy() for v in tqdm(savedmodel.variables) if v.name in qa_vars])\n",
    "model.set_weights([v.numpy() for v in tqdm(savedmodel.variables)])\n",
    "del savedmodel\n",
    "# encoder.trainable = False\n",
    "# qa_head.trainable = False\n",
    "model.trainable = False\n",
    "\n",
    "# loading hf weights\n",
    "hf_model = BigBirdForQuestionAnswering.from_pretrained(f\"google/{model_id}\")\n",
    "hf_model.eval()\n",
    "\n",
    "\"model weights loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd(input_ids, question_lengths):\n",
    "    encoder_out = model.encoder(dict(\n",
    "        input_word_ids=input_ids,\n",
    "        input_mask=tf.cast(input_ids > 0, tf.int32),\n",
    "        input_type_ids=1 - tf.sequence_mask(question_lengths, seqlen, tf.int32)\n",
    "    ))\n",
    "    out = model.qa_head(dict(\n",
    "        token_embeddings=encoder_out[\"sequence_output\"], \n",
    "        token_ids=input_ids,\n",
    "        question_lengths=question_lengths,\n",
    "    ))\n",
    "    return out, encoder_out[\"sequence_output\"]\n",
    "\n",
    "# out, sequence_output = fwd(input_ids, question_lengths)\n",
    "# start_logits, end_logits = out[:,:,0], out[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.nlp.projects.bigbird import attention\n",
    "\n",
    "\n",
    "word_ids = input_ids\n",
    "mask = tf.cast(input_ids > 0, tf.int32)\n",
    "type_ids = 1 - tf.sequence_mask(question_lengths, seqlen, tf.int32)\n",
    "\n",
    "word_embeddings = model.encoder._embedding_layer(word_ids)\n",
    "position_embeddings = model.encoder._position_embedding_layer(word_embeddings)\n",
    "type_embeddings = model.encoder._type_embedding_layer(type_ids)\n",
    "embeddings = tf.keras.layers.Add()(\n",
    "        [word_embeddings, position_embeddings, type_embeddings])\n",
    "block_size = model.encoder.get_config()[\"block_size\"]\n",
    "num_layers = model.encoder.get_config()[\"num_layers\"]\n",
    "embeddings = model.encoder._embedding_norm_layer(embeddings)\n",
    "\n",
    "data = embeddings\n",
    "masks = attention.BigBirdMasks(block_size=block_size)(\n",
    "        tf.cast(mask, embeddings.dtype))\n",
    "l1_input = data\n",
    "l = []\n",
    "for i in range(num_layers):\n",
    "    data = model.encoder._transformer_layers[i]([data, masks])\n",
    "    l.append(data)\n",
    "sequence_output=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model = BigBirdForQuestionAnswering.from_pretrained(f\"google/{model_id}\", block_size=16)\n",
    "hf_start_logits, hf_end_logits = hf_model(hf_input_ids).to_tuple()\n",
    "hf_sequence_output = hf_model.encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw input_ids: 0\n",
      "difference bw token_type_ids: 0\n",
      "difference bw word embeddings: 0.0\n",
      "difference bw position embeddings: 0.0\n",
      "difference bw token type embeddings: 0.0\n",
      "difference bw embeddings: 1.4305115e-06\n",
      "difference bw l1 layer_output 2.632723\n",
      "difference bw last layer_output 8.78882\n",
      "difference bw encoder sequence out 8.78882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"difference bw input_ids:\", difference_between_tensors(input_ids, hf_input_ids))\n",
    "\n",
    "print(\"difference bw token_type_ids:\", difference_between_tensors(type_ids, hf_model.tti))\n",
    "\n",
    "print(\"difference bw word embeddings:\", difference_between_tensors(word_embeddings, hf_model.bert.embeddings.we))\n",
    "\n",
    "print(\"difference bw position embeddings:\", difference_between_tensors(position_embeddings, hf_model.bert.embeddings.pe))\n",
    "\n",
    "print(\"difference bw token type embeddings:\", difference_between_tensors(type_embeddings, hf_model.bert.embeddings.tte))\n",
    "\n",
    "\n",
    "print(\"difference bw embeddings:\", difference_between_tensors(embeddings, hf_model.bert.embed))\n",
    "\n",
    "print(\"difference bw l1 layer_output\", difference_between_tensors(l[0], hf_model.bert.encoder.l[0]))\n",
    "print(\"difference bw last layer_output\", difference_between_tensors(l[-1],hf_model.bert.encoder.l[-1]))\n",
    "\n",
    "print(\"difference bw encoder sequence out\", difference_between_tensors(sequence_output, hf_sequence_output), end=\"\\n\\n\")\n",
    "\n",
    "# print(\"difference bw bigbird-qa logits\", difference_between_tensors(pooler_output, hf_pooler_output), end=\"\\n\\n\")\n",
    "\n",
    "# print(\"difference bw bigbird masked_lm_log_probs\", difference_between_tensors(masked_lm_log_probs, hf_masked_lm_log_probs), end=\"\\n\\n\")\n",
    "# print(\"difference bw bigbird next_sentence_log_probs\", difference_between_tensors(next_sentence_log_probs, hf_next_sentence_log_probs), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw l1 attn out 13.5019865\n"
     ]
    }
   ],
   "source": [
    "# deep inside attention layer\n",
    "\n",
    "print(\"difference bw l1 attn out\", difference_between_tensors(model.encoder._transformer_layers[0].ao, hf_model.bert.encoder.layer[0].ao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw q 14.015392\ndifference bw k 13.797251\ndifference bw v 3.112678\n"
     ]
    }
   ],
   "source": [
    "print(\"difference bw q\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.q, hf_model.bert.encoder.layer[0].attention.self.q))\n",
    "\n",
    "print(\"difference bw k\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.k, hf_model.bert.encoder.layer[0].attention.self.k))\n",
    "\n",
    "print(\"difference bw v\",difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.v, hf_model.bert.encoder.layer[0].attention.self.v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([1, 1024, 12, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.encoder._transformer_layers[0]._attention_layer.v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 12, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "hf_model.bert.encoder.layer[0].attention.self.v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.4305115e-06"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "difference_between_tensors(model.encoder._transformer_layers[0].it, hf_model.bert.encoder.layer[0].attention.self.it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.1864133"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "difference_between_tensors(tf.reshape(model.encoder._transformer_layers[0]._attention_layer.clo, (1, 1024, -1)), hf_model.bert.encoder.layer[0].attention.self.clo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "hf_model.bert.encoder.layer[0].attention.self.clo.view(1, 1024, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('transformers': conda)",
   "metadata": {
    "interpreter": {
     "hash": "43b8762cdba22b1f3661f53828ffc27c829b0d988d5ae49721f2db103a874ee7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}