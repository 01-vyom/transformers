{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import official.nlp.projects.triviaqa.modeling as modeling\n",
    "from official.nlp.configs.encoders import EncoderConfig, build_encoder\n",
    "import os\n",
    "\n",
    "from transformers import BigBirdConfig, BigBirdModel, BigBirdForQuestionAnswering\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def difference_between_tensors(tf_tensor, pt_tensor):\n",
    "    tf_np = np.array(tf_tensor)\n",
    "    pt_np = np.array(pt_tensor.detach())\n",
    "    return np.max(np.abs(tf_np - pt_np))\n",
    "\n",
    "model_id = \"bigbird-base-trivia-itc\"\n",
    "TF_CKPT_DIR = f\"ckpt/{model_id}/model.ckpt-0\"\n",
    "HF_CKPT_DIR = f\"google/{model_id}/pytorch_model.bin\""
   ]
  },
  {
   "source": [
    "seqlen = 1024 # min seqlen we can keep in this case\n",
    "config = EncoderConfig(type=\"bigbird\")\n",
    "config.bigbird.block_size = 16\n",
    "hf_config = BigBirdConfig(num_hidden_layers=config.bigbird.num_layers, hidden_act=\"gelu_fast\", attention_type=\"block_sparse\", num_random_blocks=config.bigbird.num_rand_blocks, **config.bigbird.__dict__)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "arr = np.random.randint(1, seqlen, size=seqlen).reshape(1, seqlen)\n",
    "sep_pos = 9\n",
    "arr[:, sep_pos] = 66 # sep_id\n",
    "\n",
    "input_ids = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
    "question_lengths = tf.constant([sep_pos+1], dtype=tf.int32)\n",
    "\n",
    "hf_input_ids = torch.from_numpy(arr).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 685,  560,  630,  193,  836,  764,  708,  360,   10,   66,  278,\n",
       "        755,  805,  600,   71,  473,  601,  397,  315,  706,  487,  552,\n",
       "         88,  175,  601,  850,  678,  538,  846,   73,  778,  917,  116,\n",
       "        977,  756,  710, 1023,  848,  432,  449,  851,  100,  985,  178,\n",
       "        756,  798,  660,  148,  911,  424,  289,  962,  266,  698,  640,\n",
       "        545,  544,  715,  245,  152,  676,  511,  460,  883,  184,   29,\n",
       "        803,  129,  129,  933,   54,  902,  551,  489,  757,  274,  336,\n",
       "        389,  618,   43,  443,  544,  889,  258,  322, 1000,  938,   58,\n",
       "        292,  871,  120,  780,  431,   83,   92,  897,  399,  612,  566,\n",
       "        909,  634,  939,   85,  204,  325,  775,  965,   48,  640, 1013,\n",
       "        132,  973,  869,  181, 1001,  847,  144,  661,  228,  955,  792,\n",
       "        720,  910,  374,  854,  561,  306,  582], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "input_ids.numpy()[0, :128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:Encoder class: BigBirdEncoder to build...\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f8469e88280>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_1', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f842c0891f0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_2', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f852edb3fa0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_3', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f842b548670>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_4', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f842b548d30>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_5', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f842c199e80>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_6', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f842c34b9d0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_7', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f842c442400>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_8', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f842c0326d0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_9', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f842c2e89a0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_10', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f8433452ca0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_11', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f84360fbf70>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f852cab0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "100%|██████████| 205/205 [00:00<00:00, 614.35it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model weights loaded'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# loading tf weights\n",
    "savedmodel = tf.saved_model.load(os.path.join(\"ckpt\", model_id))\n",
    "model = modeling.TriviaQaModel(config, seqlen)\n",
    "#encoder = build_encoder(config)\n",
    "# qa_head = modeling.TriviaQaHead(\n",
    "#         config.get().intermediate_size,\n",
    "#         dropout_rate=config.get().dropout_rate,\n",
    "#         attention_dropout_rate=config.get().attention_dropout_rate)\n",
    "\n",
    "_ = model(dict(\n",
    "    token_ids=input_ids,\n",
    "    question_lengths=question_lengths\n",
    "))\n",
    "\n",
    "# x = model.encoder(dict(\n",
    "#     input_word_ids=inputs['token_ids'],\n",
    "#     input_mask=tf.cast(inputs['token_ids'] > 0, tf.int32),\n",
    "#     input_type_ids=1 - tf.sequence_mask(inputs['question_lengths'], seqlen, tf.int32)\n",
    "# ))\n",
    "\n",
    "# _ = model.qa_head(dict(\n",
    "#     token_embeddings=x['sequence_output'], \n",
    "#     token_ids=inputs['token_ids'],\n",
    "#     question_lengths=inputs['question_lengths']\n",
    "# ))\n",
    "\n",
    "# enc_vars = [v.name for v in encoder.variables]\n",
    "# qa_vars = [v.name for v in qa_head.variables]\n",
    "\n",
    "# encoder.set_weights([v.numpy() for v in tqdm(savedmodel.variables) if v.name in enc_vars])\n",
    "# qa_head.set_weights([v.numpy() for v in tqdm(savedmodel.variables) if v.name in qa_vars])\n",
    "model.set_weights([v.numpy() for v in tqdm(savedmodel.variables)])\n",
    "del savedmodel\n",
    "# encoder.trainable = False\n",
    "# qa_head.trainable = False\n",
    "model.trainable = False\n",
    "\n",
    "# loading hf weights\n",
    "hf_model = BigBirdForQuestionAnswering.from_pretrained(f\"google/{model_id}\")\n",
    "hf_model.eval()\n",
    "\n",
    "\"model weights loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def fwd(input_ids, question_lengths):\n",
    "    encoder_out = model.encoder(dict(\n",
    "        input_word_ids=input_ids,\n",
    "        input_mask=tf.cast(input_ids > 0, tf.int32),\n",
    "        input_type_ids=1 - tf.sequence_mask(question_lengths, seqlen, tf.int32)\n",
    "    ))\n",
    "    out = model.qa_head(dict(\n",
    "        token_embeddings=encoder_out[\"sequence_output\"], \n",
    "        token_ids=input_ids,\n",
    "        question_lengths=question_lengths,\n",
    "    ))\n",
    "    return out, encoder_out[\"sequence_output\"]\n",
    "\n",
    "out, sequence_output = fwd(input_ids, question_lengths)\n",
    "start_logits, end_logits = out[:,:,0], out[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model = BigBirdForQuestionAnswering.from_pretrained(f\"google/{model_id}\", block_size=16)\n",
    "hf_start_logits, hf_end_logits = hf_model(hf_input_ids).to_tuple()\n",
    "hf_sequence_output = hf_model.encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw encoder sequence out 11.681164\n\n"
     ]
    }
   ],
   "source": [
    "# print(\"difference bw input_ids:\", difference_between_tensors(model.input_ids, hf_model.bert.input_ids))\n",
    "# print(\"difference bw word_embeddings:\", difference_between_tensors(model.word_embeddings, hf_model.bert.word_embeddings))\n",
    "\n",
    "# print(\"difference bw l1 layer_input\", difference_between_tensors(model.encoder.l1_layer_input, hf_model.bert.encoder.l1_layer_input))\n",
    "\n",
    "# print(\"difference bw l1 layer_output\", difference_between_tensors(model.encoder.l1_layer_output, hf_model.bert.encoder.l1_layer_output))\n",
    "# print(\"difference bw last layer_output\", difference_between_tensors(model.encoder.last_layer_output,hf_model.bert.encoder.last_layer_output))\n",
    "\n",
    "print(\"difference bw encoder sequence out\", difference_between_tensors(sequence_output, hf_sequence_output), end=\"\\n\\n\")\n",
    "\n",
    "# print(\"difference bw bigbird-qa logits\", difference_between_tensors(pooler_output, hf_pooler_output), end=\"\\n\\n\")\n",
    "\n",
    "# print(\"difference bw bigbird masked_lm_log_probs\", difference_between_tensors(masked_lm_log_probs, hf_masked_lm_log_probs), end=\"\\n\\n\")\n",
    "# print(\"difference bw bigbird next_sentence_log_probs\", difference_between_tensors(next_sentence_log_probs, hf_next_sentence_log_probs), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024, 768]),\n",
       " tensor([[[-1.0275e-01,  1.2287e-01,  8.9848e-02,  ...,  6.8178e-01,\n",
       "           -2.5683e-01,  2.1847e-02],\n",
       "          [-2.1408e-01,  1.1707e-01,  1.0406e-01,  ...,  6.1583e-01,\n",
       "           -2.1482e-01,  2.6167e-03],\n",
       "          [-2.2626e-01,  1.0723e-01,  9.2723e-02,  ...,  5.8291e-01,\n",
       "           -2.0993e-01,  1.2698e-02],\n",
       "          ...,\n",
       "          [-1.3344e-02,  4.8234e-02,  6.4713e-02,  ...,  6.3735e-01,\n",
       "           -2.0217e-01,  4.9484e-03],\n",
       "          [-1.4869e-01,  1.2185e-01,  8.7017e-02,  ...,  6.6509e-01,\n",
       "           -2.1420e-01,  4.0150e-04],\n",
       "          [-1.7408e-01,  1.4138e-01,  1.2934e-01,  ...,  7.4175e-01,\n",
       "           -2.4669e-01, -1.9718e-02]]], grad_fn=<NativeLayerNormBackward>))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "hf_sequence_output.shape, hf_sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1024, 768), dtype=float32, numpy=\n",
       "array([[[-0.6282492 ,  0.29299787, -0.5213406 , ...,  0.44641694,\n",
       "         -0.43187696, -0.1144705 ],\n",
       "        [-0.6360485 , -0.12935445, -0.1261341 , ...,  0.31974655,\n",
       "         -0.09079605,  0.04832615],\n",
       "        [-0.46901706,  0.06238546, -0.41553906, ..., -0.00190075,\n",
       "          0.1418799 , -0.11939595],\n",
       "        ...,\n",
       "        [ 0.32888898, -0.1809986 , -0.39110774, ...,  0.5249772 ,\n",
       "          0.27071732,  0.17822777],\n",
       "        [-0.4122319 ,  0.08259138, -0.28863332, ...,  0.41247615,\n",
       "          0.02627292, -0.08267803],\n",
       "        [-0.40902504,  0.17081302, -0.15940398, ...,  0.70709133,\n",
       "          0.02209761, -0.09360607]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('transformers': conda)",
   "metadata": {
    "interpreter": {
     "hash": "43b8762cdba22b1f3661f53828ffc27c829b0d988d5ae49721f2db103a874ee7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}