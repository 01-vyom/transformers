{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import official.nlp.projects.triviaqa.modeling as modeling\n",
    "from official.nlp.configs.encoders import EncoderConfig, build_encoder\n",
    "import os\n",
    "\n",
    "from transformers import BigBirdConfig, BigBirdModel, BigBirdForQuestionAnswering\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def difference_between_tensors(tf_tensor, pt_tensor):\n",
    "    tf_np = np.array(tf_tensor)\n",
    "    pt_np = np.array(pt_tensor.detach())\n",
    "    return np.max(np.abs(tf_np - pt_np))\n",
    "\n",
    "model_id = \"bigbird-base-trivia-itc\""
   ]
  },
  {
   "source": [
    "seqlen = 1024 # min seqlen we can keep in this case\n",
    "config = EncoderConfig(type=\"bigbird\")\n",
    "config.bigbird.block_size = 16\n",
    "hf_config = BigBirdConfig(num_hidden_layers=config.bigbird.num_layers, hidden_act=\"gelu_fast\", attention_type=\"block_sparse\", num_random_blocks=config.bigbird.num_rand_blocks, **config.bigbird.__dict__)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "arr = np.random.randint(1, seqlen, size=seqlen).reshape(1, seqlen)\n",
    "sep_pos = 9\n",
    "arr[:, sep_pos] = 66 # sep_id\n",
    "\n",
    "input_ids = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
    "question_lengths = tf.constant([sep_pos+1], dtype=tf.int32)\n",
    "\n",
    "hf_input_ids = torch.from_numpy(arr).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:Encoder class: BigBirdEncoder to build...\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f9554d80280>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_1', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f95220a7f70>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_2', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f95220aa190>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_3', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f9521fc72b0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_4', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f952143aee0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_5', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f952144a0d0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_6', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f9521e9afd0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_7', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f9521e9e040>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_8', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f952230e550>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_9', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f952de6fa60>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_10', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f9531719fa0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_11', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f9533f4f4f0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f9622af0af0>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "100%|██████████| 205/205 [00:00<00:00, 468.91it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model weights loaded'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# loading tf weights\n",
    "savedmodel = tf.saved_model.load(os.path.join(\"ckpt\", model_id))\n",
    "model = modeling.TriviaQaModel(config, seqlen)\n",
    "#encoder = build_encoder(config)\n",
    "# qa_head = modeling.TriviaQaHead(\n",
    "#         config.get().intermediate_size,\n",
    "#         dropout_rate=config.get().dropout_rate,\n",
    "#         attention_dropout_rate=config.get().attention_dropout_rate)\n",
    "\n",
    "_ = model(dict(\n",
    "    token_ids=input_ids,\n",
    "    question_lengths=question_lengths\n",
    "))\n",
    "\n",
    "# x = model.encoder(dict(\n",
    "#     input_word_ids=inputs['token_ids'],\n",
    "#     input_mask=tf.cast(inputs['token_ids'] > 0, tf.int32),\n",
    "#     input_type_ids=1 - tf.sequence_mask(inputs['question_lengths'], seqlen, tf.int32)\n",
    "# ))\n",
    "\n",
    "# _ = model.qa_head(dict(\n",
    "#     token_embeddings=x['sequence_output'], \n",
    "#     token_ids=inputs['token_ids'],\n",
    "#     question_lengths=inputs['question_lengths']\n",
    "# ))\n",
    "\n",
    "# enc_vars = [v.name for v in encoder.variables]\n",
    "# qa_vars = [v.name for v in qa_head.variables]\n",
    "\n",
    "# encoder.set_weights([v.numpy() for v in tqdm(savedmodel.variables) if v.name in enc_vars])\n",
    "# qa_head.set_weights([v.numpy() for v in tqdm(savedmodel.variables) if v.name in qa_vars])\n",
    "model.set_weights([v.numpy() for v in tqdm(savedmodel.variables)])\n",
    "del savedmodel\n",
    "# encoder.trainable = False\n",
    "# qa_head.trainable = False\n",
    "model.trainable = False\n",
    "\n",
    "# loading hf weights\n",
    "hf_model = BigBirdForQuestionAnswering(hf_config)\n",
    "state_dict = torch.load(f\"google/{model_id}/pytorch_model.bin\")\n",
    "hf_model.load_state_dict(state_dict)\n",
    "hf_model.eval()\n",
    "\n",
    "\"model weights loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD BIGBIRD QA MODEL\n",
    "\n",
    "from official.nlp.projects.bigbird import attention\n",
    "\n",
    "\n",
    "word_ids = input_ids\n",
    "mask = tf.cast(input_ids > 0, tf.int32)\n",
    "type_ids = 1 - tf.sequence_mask(question_lengths, seqlen, tf.int32)\n",
    "\n",
    "word_embeddings = model.encoder._embedding_layer(word_ids)\n",
    "position_embeddings = model.encoder._position_embedding_layer(word_embeddings)\n",
    "type_embeddings = model.encoder._type_embedding_layer(type_ids)\n",
    "embeddings = tf.keras.layers.Add()(\n",
    "        [word_embeddings, position_embeddings, type_embeddings])\n",
    "block_size = model.encoder.get_config()[\"block_size\"]\n",
    "num_layers = model.encoder.get_config()[\"num_layers\"]\n",
    "embeddings = model.encoder._embedding_norm_layer(embeddings)\n",
    "\n",
    "data = embeddings\n",
    "masks = attention.BigBirdMasks(block_size=block_size)(\n",
    "        tf.cast(mask, embeddings.dtype))\n",
    "l1_input = data\n",
    "l = []\n",
    "for i in range(num_layers):\n",
    "    data = model.encoder._transformer_layers[i]([data, masks])\n",
    "    l.append(data)\n",
    "sequence_output=data\n",
    "\n",
    "out = model.qa_head(dict(\n",
    "        token_embeddings=sequence_output, \n",
    "        token_ids=input_ids,\n",
    "        question_lengths=question_lengths,\n",
    "    ))\n",
    "start_logits, end_logits = out[:,:,0], out[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model = BigBirdForQuestionAnswering.from_pretrained(f\"google/{model_id}\", block_size=16)\n",
    "hf_start_logits, hf_end_logits = hf_model(hf_input_ids).to_tuple()\n",
    "hf_sequence_output = hf_model.encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw input_ids: 0\ndifference bw token_type_ids: 0\ndifference bw word embeddings: 0.0\ndifference bw position embeddings: 0.0\ndifference bw token type embeddings: 0.0\ndifference bw embeddings: 1.4305115e-06\ndifference bw l1 layer_output 2.3841858e-06\ndifference bw last layer_output 0.00012207031\ndifference bw encoder sequence out 0.00012207031\n\ndifference bw start logits 0.020858765\n\ndifference bw end logits 0.0625\n\n"
     ]
    }
   ],
   "source": [
    "print(\"difference bw input_ids:\", difference_between_tensors(input_ids, hf_input_ids))\n",
    "\n",
    "print(\"difference bw token_type_ids:\", difference_between_tensors(type_ids, hf_model.tti))\n",
    "\n",
    "print(\"difference bw word embeddings:\", difference_between_tensors(word_embeddings, hf_model.bert.embeddings.we))\n",
    "\n",
    "print(\"difference bw position embeddings:\", difference_between_tensors(position_embeddings, hf_model.bert.embeddings.pe))\n",
    "\n",
    "print(\"difference bw token type embeddings:\", difference_between_tensors(type_embeddings, hf_model.bert.embeddings.tte))\n",
    "\n",
    "print(\"difference bw embeddings:\", difference_between_tensors(embeddings, hf_model.bert.embed))\n",
    "\n",
    "print(\"difference bw l1 layer_output\", difference_between_tensors(l[0], hf_model.bert.encoder.l[0]))\n",
    "print(\"difference bw last layer_output\", difference_between_tensors(l[-1],hf_model.bert.encoder.l[-1]))\n",
    "\n",
    "print(\"difference bw encoder sequence out\", difference_between_tensors(sequence_output, hf_sequence_output), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw start logits\", difference_between_tensors(start_logits, hf_start_logits), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw end logits\", difference_between_tensors(end_logits, hf_end_logits), end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw encoder out 0.00012207031\n\ndifference bw inter 7.390976e-06\n\ndifference bw o1 2.0980835e-05\n\ndifference bw it 0.00012207031\n\ndifference bw hs 2.0980835e-05\n\ndifference bw o 0.011413574\n\ndifference bw l 0.021133423\n\ndifference bw final logits 0.0625\n\ndifference bw lmask 0.0\n\n"
     ]
    }
   ],
   "source": [
    "print(\"difference bw encoder out\", difference_between_tensors(model.qa_head.qa_te, hf_model.qa_classifier.qa_te), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw inter\", difference_between_tensors(model.qa_head.inter, hf_model.qa_classifier.inter), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw o1\", difference_between_tensors(model.qa_head.o_1, hf_model.qa_classifier.output.o_1), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw it\", difference_between_tensors(model.qa_head.it, hf_model.qa_classifier.output.it), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw hs\", difference_between_tensors(model.qa_head.hs, hf_model.qa_classifier.output.hs), end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"difference bw o\", difference_between_tensors(model.qa_head.o, hf_model.qa_classifier.o), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw l\", difference_between_tensors(model.qa_head.l, hf_model.l), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw final logits\", difference_between_tensors(out, hf_model.fl), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw lmask\", difference_between_tensors(model.qa_head.lmask, hf_model.lmask), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "difference_between_tensors(model.variables[-5], hf_model.state_dict()[\"qa_classifier.output.LayerNorm.bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf_q = model.encoder._transformer_layers[0]._attention_layer.q\n",
    "# py_q = hf_model.bert.encoder.layer[0].attention.self.q\n",
    "\n",
    "# print(\"difference bw q\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.q, hf_model.bert.encoder.layer[0].attention.self.q))\n",
    "\n",
    "# print(\"difference bw k\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.k, hf_model.bert.encoder.layer[0].attention.self.k))\n",
    "\n",
    "# print(\"difference bw v\",difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.v, hf_model.bert.encoder.layer[0].attention.self.v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deep inside attention layer\n",
    "\n",
    "# print(\"difference bw l1 attn out\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.clo, hf_model.bert.encoder.layer[0].attention.self.clo))\n",
    "\n",
    "# print(\"difference bw bqm\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.bqm, hf_model.bert.encoder.layer[0].attention.self.bqm))\n",
    "\n",
    "# print(\"difference bw bkm\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.bkm, hf_model.bert.encoder.layer[0].attention.self.bkm))\n",
    "\n",
    "# print(\"difference bw bvm\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.bvm, hf_model.bert.encoder.layer[0].attention.self.bvm))\n",
    "\n",
    "# print(\"difference bw ra\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.ra, hf_model.bert.encoder.layer[0].attention.self.ra))\n",
    "\n",
    "# print(\"difference bw fcl\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.fcl, hf_model.bert.encoder.layer[0].attention.self.fcl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('transformers': conda)",
   "metadata": {
    "interpreter": {
     "hash": "43b8762cdba22b1f3661f53828ffc27c829b0d988d5ae49721f2db103a874ee7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}