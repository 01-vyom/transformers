{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import official.nlp.projects.triviaqa.modeling as modeling\n",
    "from official.nlp.configs.encoders import EncoderConfig, build_encoder\n",
    "import os\n",
    "\n",
    "from transformers import BigBirdConfig, BigBirdModel, BigBirdForQuestionAnswering\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def difference_between_tensors(tf_tensor, pt_tensor):\n",
    "    tf_np = np.array(tf_tensor)\n",
    "    pt_np = np.array(pt_tensor.detach())\n",
    "    return np.max(np.abs(tf_np - pt_np))\n",
    "\n",
    "model_id = \"bigbird-base-trivia-itc\""
   ]
  },
  {
   "source": [
    "seqlen = 1024 # min seqlen we can keep in this case\n",
    "config = EncoderConfig(type=\"bigbird\")\n",
    "config.bigbird.block_size = 16\n",
    "hf_config = BigBirdConfig(num_hidden_layers=config.bigbird.num_layers, hidden_act=\"gelu_fast\", attention_type=\"block_sparse\", num_random_blocks=config.bigbird.num_rand_blocks, **config.bigbird.__dict__)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "arr = np.random.randint(1, seqlen, size=seqlen).reshape(1, seqlen)\n",
    "sep_pos = 9\n",
    "arr[:, sep_pos] = 66 # sep_id\n",
    "\n",
    "input_ids = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
    "question_lengths = tf.constant([sep_pos+1], dtype=tf.int32)\n",
    "\n",
    "hf_input_ids = torch.from_numpy(arr).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:Encoder class: BigBirdEncoder to build...\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f964ce89a90>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_1', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f960e437b20>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_2', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f960f06a580>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_3', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f960e422880>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_4', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f960ee9c0a0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_5', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f960f0f1850>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_6', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f960f366460>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_7', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f960f004970>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_8', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f960f153e80>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_9', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f9610ee63d0>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_10', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f961d2de910>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "INFO:absl:TransformerScaffold configs: {'name': 'transformer_scaffold_11', 'trainable': True, 'dtype': 'float32', 'attention_cls': <official.nlp.projects.bigbird.attention.BigBirdAttention object at 0x7f961efc7e20>, 'feedforward_cls': None, 'num_attention_heads': 12, 'intermediate_size': 3072, 'intermediate_activation': <function gelu at 0x7f970fa71b80>, 'dropout_rate': 0.1, 'attention_dropout_rate': 0.1, 'norm_first': False, 'kernel_initializer': {'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "100%|██████████| 205/205 [00:00<00:00, 576.50it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model weights loaded'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# loading tf weights\n",
    "savedmodel = tf.saved_model.load(os.path.join(\"ckpt\", model_id))\n",
    "model = modeling.TriviaQaModel(config, seqlen)\n",
    "#encoder = build_encoder(config)\n",
    "# qa_head = modeling.TriviaQaHead(\n",
    "#         config.get().intermediate_size,\n",
    "#         dropout_rate=config.get().dropout_rate,\n",
    "#         attention_dropout_rate=config.get().attention_dropout_rate)\n",
    "\n",
    "_ = model(dict(\n",
    "    token_ids=input_ids,\n",
    "    question_lengths=question_lengths\n",
    "))\n",
    "\n",
    "# x = model.encoder(dict(\n",
    "#     input_word_ids=inputs['token_ids'],\n",
    "#     input_mask=tf.cast(inputs['token_ids'] > 0, tf.int32),\n",
    "#     input_type_ids=1 - tf.sequence_mask(inputs['question_lengths'], seqlen, tf.int32)\n",
    "# ))\n",
    "\n",
    "# _ = model.qa_head(dict(\n",
    "#     token_embeddings=x['sequence_output'], \n",
    "#     token_ids=inputs['token_ids'],\n",
    "#     question_lengths=inputs['question_lengths']\n",
    "# ))\n",
    "\n",
    "# enc_vars = [v.name for v in encoder.variables]\n",
    "# qa_vars = [v.name for v in qa_head.variables]\n",
    "\n",
    "# encoder.set_weights([v.numpy() for v in tqdm(savedmodel.variables) if v.name in enc_vars])\n",
    "# qa_head.set_weights([v.numpy() for v in tqdm(savedmodel.variables) if v.name in qa_vars])\n",
    "model.set_weights([v.numpy() for v in tqdm(savedmodel.variables)])\n",
    "del savedmodel\n",
    "# encoder.trainable = False\n",
    "# qa_head.trainable = False\n",
    "model.trainable = False\n",
    "\n",
    "# loading hf weights\n",
    "hf_model = BigBirdForQuestionAnswering(hf_config)\n",
    "state_dict = torch.load(f\"google/{model_id}/pytorch_model.bin\")\n",
    "hf_model.load_state_dict(state_dict)\n",
    "hf_model.eval()\n",
    "\n",
    "\"model weights loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd(input_ids, question_lengths):\n",
    "    encoder_out = model.encoder(dict(\n",
    "        input_word_ids=input_ids,\n",
    "        input_mask=tf.cast(input_ids > 0, tf.int32),\n",
    "        input_type_ids=1 - tf.sequence_mask(question_lengths, seqlen, tf.int32)\n",
    "    ))\n",
    "    out = model.qa_head(dict(\n",
    "        token_embeddings=encoder_out[\"sequence_output\"], \n",
    "        token_ids=input_ids,\n",
    "        question_lengths=question_lengths,\n",
    "    ))\n",
    "    return out, encoder_out[\"sequence_output\"]\n",
    "\n",
    "# out, sequence_output = fwd(input_ids, question_lengths)\n",
    "# start_logits, end_logits = out[:,:,0], out[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.nlp.projects.bigbird import attention\n",
    "\n",
    "\n",
    "word_ids = input_ids\n",
    "mask = tf.cast(input_ids > 0, tf.int32)\n",
    "type_ids = 1 - tf.sequence_mask(question_lengths, seqlen, tf.int32)\n",
    "\n",
    "word_embeddings = model.encoder._embedding_layer(word_ids)\n",
    "position_embeddings = model.encoder._position_embedding_layer(word_embeddings)\n",
    "type_embeddings = model.encoder._type_embedding_layer(type_ids)\n",
    "embeddings = tf.keras.layers.Add()(\n",
    "        [word_embeddings, position_embeddings, type_embeddings])\n",
    "block_size = model.encoder.get_config()[\"block_size\"]\n",
    "num_layers = model.encoder.get_config()[\"num_layers\"]\n",
    "embeddings = model.encoder._embedding_norm_layer(embeddings)\n",
    "\n",
    "data = embeddings\n",
    "masks = attention.BigBirdMasks(block_size=block_size)(\n",
    "        tf.cast(mask, embeddings.dtype))\n",
    "l1_input = data\n",
    "l = []\n",
    "for i in range(num_layers):\n",
    "    data = model.encoder._transformer_layers[i]([data, masks])\n",
    "    l.append(data)\n",
    "sequence_output=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model = BigBirdForQuestionAnswering.from_pretrained(f\"google/{model_id}\", block_size=16)\n",
    "hf_start_logits, hf_end_logits = hf_model(hf_input_ids).to_tuple()\n",
    "hf_sequence_output = hf_model.encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw input_ids: 0\ndifference bw embeddings: 1.4305115e-06\ndifference bw l1 layer_output 3.1019783\ndifference bw last layer_output 11.384542\ndifference bw encoder sequence out 11.384542\n\n"
     ]
    }
   ],
   "source": [
    "print(\"difference bw input_ids:\", difference_between_tensors(input_ids, hf_input_ids))\n",
    "\n",
    "# print(\"difference bw token_type_ids:\", difference_between_tensors(type_ids, hf_model.tti))\n",
    "\n",
    "# print(\"difference bw word embeddings:\", difference_between_tensors(word_embeddings, hf_model.bert.embeddings.we))\n",
    "\n",
    "# print(\"difference bw position embeddings:\", difference_between_tensors(position_embeddings, hf_model.bert.embeddings.pe))\n",
    "\n",
    "# print(\"difference bw token type embeddings:\", difference_between_tensors(type_embeddings, hf_model.bert.embeddings.tte))\n",
    "\n",
    "\n",
    "print(\"difference bw embeddings:\", difference_between_tensors(embeddings, hf_model.bert.embed))\n",
    "\n",
    "print(\"difference bw l1 layer_output\", difference_between_tensors(l[0], hf_model.bert.encoder.l[0]))\n",
    "print(\"difference bw last layer_output\", difference_between_tensors(l[-1],hf_model.bert.encoder.l[-1]))\n",
    "\n",
    "print(\"difference bw encoder sequence out\", difference_between_tensors(sequence_output, hf_sequence_output), end=\"\\n\\n\")\n",
    "\n",
    "# print(\"difference bw bigbird-qa logits\", difference_between_tensors(pooler_output, hf_pooler_output), end=\"\\n\\n\")\n",
    "\n",
    "# print(\"difference bw bigbird masked_lm_log_probs\", difference_between_tensors(masked_lm_log_probs, hf_masked_lm_log_probs), end=\"\\n\\n\")\n",
    "# print(\"difference bw bigbird next_sentence_log_probs\", difference_between_tensors(next_sentence_log_probs, hf_next_sentence_log_probs), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw q 5.722046e-06\ndifference bw k 6.198883e-06\ndifference bw v 1.1920929e-06\n"
     ]
    }
   ],
   "source": [
    "tf_q = model.encoder._transformer_layers[0]._attention_layer.q\n",
    "py_q = hf_model.bert.encoder.layer[0].attention.self.q\n",
    "\n",
    "print(\"difference bw q\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.q, hf_model.bert.encoder.layer[0].attention.self.q))\n",
    "\n",
    "print(\"difference bw k\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.k, hf_model.bert.encoder.layer[0].attention.self.k))\n",
    "\n",
    "print(\"difference bw v\",difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.v, hf_model.bert.encoder.layer[0].attention.self.v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw l1 attn out 2.026558e-06\ndifference bw bqm 5.722046e-06\ndifference bw bkm 6.198883e-06\ndifference bw bvm 1.1920929e-06\ndifference bw ra 0\ndifference bw ra 2.026558e-06\n"
     ]
    }
   ],
   "source": [
    "# deep inside attention layer\n",
    "\n",
    "print(\"difference bw l1 attn out\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.clo, hf_model.bert.encoder.layer[0].attention.self.clo))\n",
    "\n",
    "print(\"difference bw bqm\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.bqm, hf_model.bert.encoder.layer[0].attention.self.bqm))\n",
    "\n",
    "print(\"difference bw bkm\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.bkm, hf_model.bert.encoder.layer[0].attention.self.bkm))\n",
    "\n",
    "print(\"difference bw bvm\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.bvm, hf_model.bert.encoder.layer[0].attention.self.bvm))\n",
    "\n",
    "print(\"difference bw ra\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.ra, hf_model.bert.encoder.layer[0].attention.self.ra))\n",
    "\n",
    "print(\"difference bw fcl\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.fcl, hf_model.bert.encoder.layer[0].attention.self.fcl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw clo 2.026558e-06\ndifference bw ao 18.428173\n"
     ]
    }
   ],
   "source": [
    "print(\"difference bw clo\", difference_between_tensors(model.encoder._transformer_layers[0]._attention_layer.clo, hf_model.bert.encoder.layer[0].attention.self.clo))\n",
    "\n",
    "print(\"difference bw ao\", difference_between_tensors(model.encoder._transformer_layers[0].ao, hf_model.bert.encoder.layer[0].ao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('transformers': conda)",
   "metadata": {
    "interpreter": {
     "hash": "43b8762cdba22b1f3661f53828ffc27c829b0d988d5ae49721f2db103a874ee7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}