{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets debug bigbird\n",
    "\n",
    "# vocab is same as roberta/gpt2\n",
    "# for running `sumulated_sparse`, encoder_max seqlen must be 4096\n",
    "\n",
    "# fix pooler head on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigbird.core import modeling\n",
    "from bigbird.core import utils\n",
    "\n",
    "from transformers import BigBirdForMaskedLM, BigBirdConfig\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_CKPT_DIR = \"ckpt/bigbr_base/model.ckpt-0\"\n",
    "HF_CKPT_DIR = \"google/bigbird-base/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigbird_config = {\n",
    "      # transformer basic configs\n",
    "      \"vocab_size\": 50358,\n",
    "      \"attention_probs_dropout_prob\": 0.1,\n",
    "      \"hidden_act\": \"gelu\",\n",
    "      \"hidden_dropout_prob\": 0.1,\n",
    "      \"hidden_size\": 768,\n",
    "      \"initializer_range\": 0.02,\n",
    "      \"intermediate_size\": 3072,\n",
    "      \"max_position_embeddings\": 4096,\n",
    "      \"num_attention_heads\": 12,\n",
    "      \"num_hidden_layers\": 12,\n",
    "      \"type_vocab_size\": 2,\n",
    "      \"use_bias\": True,\n",
    "      \"rescale_embedding\": False,\n",
    "      \"scope\": \"bert\",\n",
    "      # sparse mask configs\n",
    "      \"attention_type\": \"original_full\", # \"block_sparse\" \"original_full\" \"simulated_sparse\"\n",
    "      \"norm_type\": \"postnorm\",\n",
    "      \"block_size\": 16,\n",
    "      \"num_rand_blocks\": 3,\n",
    "      # common bert configs\n",
    "      \"max_encoder_length\": 128,\n",
    "      \"batch_size\": 2,\n",
    "}\n",
    "\n",
    "hf_bigbird_config = BigBirdConfig.from_dict(bigbird_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "s1 = bigbird_config[\"batch_size\"]\n",
    "s2 = bigbird_config[\"max_encoder_length\"]\n",
    "\n",
    "arr = np.random.randint(1, s2, size=s1*s2).reshape(s1, s2)\n",
    "\n",
    "input_ids = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
    "hf_input_ids = torch.from_numpy(arr).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n"
     ]
    }
   ],
   "source": [
    "model = modeling.BertModel(bigbird_config)\n",
    "_, _ = model(input_ids, training=False) # building all the weights before setting-up :)\n",
    "\n",
    "hf_model = BigBirdForMaskedLM(hf_bigbird_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 199/199 [00:00<00:00, 244.67it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model weights loaded'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "ckpt_reader = tf.compat.v1.train.NewCheckpointReader(TF_CKPT_DIR)\n",
    "model.set_weights([ckpt_reader.get_tensor(v.name[:-2]) for v in tqdm(model.trainable_weights, position=0)])\n",
    "\n",
    "hf_model.load_state_dict(torch.load(HF_CKPT_DIR))\n",
    "hf_model.eval()\n",
    "\"model weights loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n"
     ]
    }
   ],
   "source": [
    "sequence_output, pooled_output = model(input_ids, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = hf_model(hf_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"model input_ids\", model.input_ids, end=\"\\n\\n\")\n",
    "# print(\"embeddding\", model.word_embeddings, end=\"\\n\\n\")\n",
    "\n",
    "# print(\"l1 layer_input\", model.encoder.l1_layer_input, end=\"\\n\\n\")\n",
    "# print(\"l1 attn_mask\", model.encoder.l1_attention_mask, end=\"\\n\\n\")\n",
    "# print(\"l1 encoder_from_mask\", model.encoder.l1_encoder_from_mask, end=\"\\n\\n\")\n",
    "# print(\"l1 encoder_to_mask\", model.encoder.l1_encoder_to_mask, end=\"\\n\\n\")\n",
    "# print(\"l1 blocked_encoder_mask\", model.encoder.l1_blocked_encoder_mask, end=\"\\n\\n\")\n",
    "# print(\"l1_training\", model.encoder.l1_training, end=\"\\n\\n\")\n",
    "\n",
    "# print(\"l1 layer_output\", model.encoder.l1_layer_output, end=\"\\n\\n\")\n",
    "# print(\"last layer_output\", model.encoder.last_layer_output, end=\"\\n\\n\")\n",
    "\n",
    "# print(\"bigbird sequence out\", sequence_output, end=\"\\n\\n\")\n",
    "# print(\"bigbird pooled output\", pooled_output, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_between_tensors(tf_tensor, pt_tensor):\n",
    "    tf_np = np.array(tf_tensor)\n",
    "    pt_np = np.array(pt_tensor.detach())\n",
    "    return np.max(np.abs(tf_np - pt_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS FOR WEIGHTS CONVERSION\n",
    "\n",
    "# from transformers import BigBirdForMaskedLM, BigBirdConfig, load_tf_weights_in_big_bird\n",
    "\n",
    "# config = BigBirdConfig()\n",
    "# model = BigBirdForMaskedLM(config)\n",
    "\n",
    "# old = model.state_dict()\n",
    "\n",
    "# model = load_tf_weights_in_big_bird(model, \"ckpt/bigbr_base/model.ckpt-0\")\n",
    "\n",
    "# model.save_pretrained(\"google/bigbird-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw input_ids: 0\ndifference bw word_embeddings: 0.0\ndifference bw l1 layer_input 7.1525574e-07\ndifference bw l1 layer_output 0.0035440922\ndifference bw last layer_output 0.0568192\n"
     ]
    }
   ],
   "source": [
    "print(\"difference bw input_ids:\", difference_between_tensors(model.input_ids, hf_model.bert.input_ids))\n",
    "print(\"difference bw word_embeddings:\", difference_between_tensors(model.word_embeddings, hf_model.bert.word_embeddings))\n",
    "\n",
    "print(\"difference bw l1 layer_input\", difference_between_tensors(model.encoder.l1_layer_input, hf_model.bert.encoder.l1_layer_input))\n",
    "\n",
    "print(\"difference bw l1 layer_output\", difference_between_tensors(model.encoder.l1_layer_output, hf_model.bert.encoder.l1_layer_output))\n",
    "print(\"difference bw last layer_output\", difference_between_tensors(model.encoder.last_layer_output,hf_model.bert.encoder.last_layer_output))\n",
    "\n",
    "# print(\"difference bw bigbird sequence out\", difference_between_tensors(sequence_output, hf_sequence_output), end=\"\\n\\n\")\n",
    "# print(\"difference bw bigbird pooled output\", difference_between_tensors(pooled_output, hf_pooled_output), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.train.list_variables(\"ckpt/bigbr_base/model.ckpt-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.big_bird.modeling_big_bird import BigBirdAttention\n",
    "# from bigbird.core.attention import MultiHeadedAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw k: 5.2452087e-06\ndifference bw q: 5.722046e-06\ndifference bw v: 1.4305115e-06\ndifference bw v: 1.4305115e-06\ndifference bw attn_sc: 3.0517578e-05\ndifference bw attn_p: 4.708767e-06\ndifference bw attn_o: 1.013279e-05\ndifference bw attn_proj_o: 8.583069e-06\ndifference bw int_o: 0.0004749298\ndifference bw io: 0.0004749298\ndifference bw o: 0.011826634\n"
     ]
    }
   ],
   "source": [
    "# layer-0 debugging\n",
    "\n",
    "print(\"difference bw k:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.k, hf_model.bert.encoder.layer[0].attention.self.k\n",
    "))\n",
    "\n",
    "print(\"difference bw q:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.q, hf_model.bert.encoder.layer[0].attention.self.q\n",
    "))\n",
    "\n",
    "print(\"difference bw v:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.v, hf_model.bert.encoder.layer[0].attention.self.v\n",
    "))\n",
    "\n",
    "print(\"difference bw v:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.v, hf_model.bert.encoder.layer[0].attention.self.v\n",
    "))\n",
    "\n",
    "print(\"difference bw attn_sc:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.attn_sc, hf_model.bert.encoder.layer[0].attention.self.attn_sc\n",
    "))\n",
    "\n",
    "print(\"difference bw attn_p:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.attn_p, hf_model.bert.encoder.layer[0].attention.self.attn_p\n",
    "))\n",
    "\n",
    "\n",
    "print(\"difference bw attn_o:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.attn_o, hf_model.bert.encoder.layer[0].attention.self.attn_o\n",
    "))\n",
    "\n",
    "print(\"difference bw attn_proj_o:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_proj_o, hf_model.bert.encoder.layer[0].attn_proj_o\n",
    "))\n",
    "\n",
    "print(\"difference bw int_o:\", difference_between_tensors(model.encoder.encoder_layers[0].int_o, hf_model.bert.encoder.layer[0].int_o\n",
    "))\n",
    "\n",
    "print(\"difference bw io:\", difference_between_tensors(model.encoder.encoder_layers[0].io, hf_model.bert.encoder.layer[0].output.io\n",
    "))\n",
    "\n",
    "print(\"difference bw o:\", difference_between_tensors(model.encoder.encoder_layers[0].o, hf_model.bert.encoder.layer[0].output.o\n",
    "))\n",
    "\n",
    "# print(\"difference bw do:\", difference_between_tensors(model.encoder.encoder_layers[0].do, hf_model.bert.encoder.layer[0].output.do\n",
    "# ))\n",
    "\n",
    "# print(\"difference bw l_o:\", difference_between_tensors(model.encoder.encoder_layers[0].l_o, hf_model.bert.encoder.layer[0].l_o\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}