{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigbird.core import modeling\n",
    "from bigbird.core import utils\n",
    "from bigbird.pretrain.run_pretraining import MaskedLMLayer, NSPLayer, serving_input_fn_builder\n",
    "\n",
    "from transformers import BigBirdForPreTraining, BigBirdConfig, BigBirdTokenizer, BigBirdForMaskedLM, BigBirdModel\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "def difference_between_tensors(tf_tensor, pt_tensor):\n",
    "    tf_np = np.array(tf_tensor)\n",
    "    pt_np = np.array(pt_tensor.detach())\n",
    "    return np.max(np.abs(tf_np - pt_np))\n",
    "\n",
    "TF_CKPT_DIR = \"ckpt/bigbr_base/model.ckpt-0\"\n",
    "HF_CKPT_DIR = \"google/bigbird-base/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ifn = serving_input_fn_builder(batch_size=1, max_encoder_length=30,\n",
    "#                     vocab_model_file=\"google/bigbird-base/gpt2.model\", substitute_newline=False)\n",
    "\n",
    "# # t = BigBirdTokenizer(\"google/bigbird-base/gpt2.model\")\n",
    "# # t.save_pretrained(\"google/bigbird-base\")\n",
    "# t = BigBirdTokenizer.from_pretrained(\"google/bigbird-base/\")\n",
    "\n",
    "# print(t([\"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"]).input_ids)\n",
    "# # ifn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigbird_config = {\n",
    "      # transformer basic configs\n",
    "      \"vocab_size\": 50358,\n",
    "      \"attention_probs_dropout_prob\": 0.1,\n",
    "      \"hidden_act\": \"gelu\",\n",
    "      \"hidden_dropout_prob\": 0.1,\n",
    "      \"hidden_size\": 768,\n",
    "      \"initializer_range\": 0.02,\n",
    "      \"intermediate_size\": 3072,\n",
    "      \"max_position_embeddings\": 4096,\n",
    "      \"num_attention_heads\": 12,\n",
    "      \"num_hidden_layers\": 12,\n",
    "      \"type_vocab_size\": 2,\n",
    "      \"use_bias\": True,\n",
    "      \"rescale_embedding\": False,\n",
    "      \"scope\": \"bert\",\n",
    "      # sparse mask configs\n",
    "      \"attention_type\": \"block_sparse\", # \"block_sparse\" \"original_full\" \"simulated_sparse\"\n",
    "      \"norm_type\": \"postnorm\",\n",
    "      \"block_size\": 128,\n",
    "      \"num_rand_blocks\": 1,\n",
    "      # common bert configs\n",
    "      \"max_encoder_length\": 1024,\n",
    "      \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "hf_bigbird_config = BigBirdConfig.from_dict(bigbird_config)\n",
    "hf_bigbird_config.hidden_act = \"gelu_fast\"\n",
    "hf_bigbird_config.num_random_blocks = bigbird_config[\"num_rand_blocks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "s1 = bigbird_config[\"batch_size\"]\n",
    "s2 = bigbird_config[\"max_encoder_length\"]\n",
    "\n",
    "arr = np.random.randint(1, s2, size=s1*s2).reshape(s1, s2)\n",
    "\n",
    "input_ids = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
    "hf_input_ids = torch.from_numpy(arr).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n"
     ]
    }
   ],
   "source": [
    "model = modeling.BertModel(bigbird_config)\n",
    "masked_lm = MaskedLMLayer(bigbird_config[\"hidden_size\"], bigbird_config[\"vocab_size\"], model.embeder, activation_fn=utils.get_activation(bigbird_config[\"hidden_act\"]))\n",
    "next_sentence = NSPLayer(bigbird_config[\"hidden_size\"])\n",
    "\n",
    "# building all the weights before setting-up :)\n",
    "sequence_output, pooler_output = model(input_ids, training=False)\n",
    "_, _ = masked_lm(sequence_output)\n",
    "_, _ = next_sentence(pooler_output)\n",
    "\n",
    "hf_model = BigBirdForPreTraining(hf_bigbird_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 199/199 [00:01<00:00, 159.60it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 350.95it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1341.10it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model weights loaded'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "ckpt_reader = tf.compat.v1.train.NewCheckpointReader(TF_CKPT_DIR)\n",
    "model.set_weights([ckpt_reader.get_tensor(v.name[:-2]) for v in tqdm(model.trainable_weights, position=0)])\n",
    "masked_lm.set_weights([ckpt_reader.get_tensor(v.name[:-2]) for v in tqdm(masked_lm.trainable_weights, position=0)])\n",
    "next_sentence.set_weights([ckpt_reader.get_tensor(v.name[:-2]) for v in tqdm(next_sentence.trainable_weights, position=0)])\n",
    "masked_lm.trainable = False\n",
    "next_sentence.trainable = False\n",
    "\n",
    "\n",
    "hf_model.load_state_dict(torch.load(HF_CKPT_DIR))\n",
    "hf_model.eval()\n",
    "\"model weights loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n",
      "INFO:absl:**** Using block sparse attention ****\n"
     ]
    }
   ],
   "source": [
    "sequence_output, pooler_output = model(input_ids, training=False)\n",
    "masked_lm_loss, masked_lm_log_probs = masked_lm(sequence_output)\n",
    "next_sentence_loss, next_sentence_log_probs = next_sentence(pooler_output)\n",
    "\n",
    "hf_out = hf_model(hf_input_ids, output_attentions=True)\n",
    "hf_sequence_output = hf_model.sequence_output\n",
    "hf_pooler_output = hf_model.pooler_output\n",
    "\n",
    "hf_masked_lm_log_probs = F.log_softmax(hf_out.prediction_logits, dim=-1)\n",
    "hf_next_sentence_log_probs = F.log_softmax(hf_out.seq_relationship_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 1024, 64]), torch.Size([1, 12, 1024, 64]))"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "ap = hf_model.bert.encoder.layer[0].attention.self.ap\n",
    "my_cl = hf_model.bert.encoder.layer[0].attention.self.my_cl\n",
    "final_cl = hf_model.bert.encoder.layer[0].attention.self.final_cl.transpose(1,2)\n",
    "my_cl.shape, final_cl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0., grad_fn=<MaxBackward1>)\ntensor(0.3152, grad_fn=<MaxBackward1>)\ntensor(0.3984, grad_fn=<MaxBackward1>)\ntensor(0.7096, grad_fn=<MaxBackward1>)\ntensor(0.2550, grad_fn=<MaxBackward1>)\ntensor(0.4019, grad_fn=<MaxBackward1>)\ntensor(1.1921e-06, grad_fn=<MaxBackward1>)\ntensor(0., grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "seqlen = bigbird_config[\"max_encoder_length\"]\n",
    "block_size = bigbird_config[\"block_size\"]\n",
    "for k in range(0, seqlen, block_size):\n",
    "    d = (my_cl[:,:,k:k+block_size,:]-final_cl[:,:,k:k+block_size,:]).max()\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.3152, grad_fn=<MaxBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "k = 1\n",
    "(my_cl[:,:,128:128*2,:]-final_cl[:,:,128:256,:]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(sequence_output.shape, hf_sequence_output.shape, pooler_output.shape, hf_pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"model input_ids\", model.input_ids, end=\"\\n\\n\")\n",
    "# print(\"embeddding\", model.word_embeddings, end=\"\\n\\n\")\n",
    "\n",
    "# print(\"l1 layer_input\", model.encoder.l1_layer_input, end=\"\\n\\n\")\n",
    "# print(\"l1 attn_mask\", model.encoder.l1_attention_mask, end=\"\\n\\n\")\n",
    "# print(\"l1 encoder_from_mask\", model.encoder.l1_encoder_from_mask, end=\"\\n\\n\")\n",
    "# print(\"l1 encoder_to_mask\", model.encoder.l1_encoder_to_mask, end=\"\\n\\n\")\n",
    "# print(\"l1 blocked_encoder_mask\", model.encoder.l1_blocked_encoder_mask, end=\"\\n\\n\")\n",
    "# print(\"l1_training\", model.encoder.l1_training, end=\"\\n\\n\")\n",
    "\n",
    "# print(\"l1 layer_output\", model.encoder.l1_layer_output, end=\"\\n\\n\")\n",
    "# print(\"last layer_output\", model.encoder.last_layer_output, end=\"\\n\\n\")\n",
    "\n",
    "# print(\"bigbird sequence out\", sequence_output, end=\"\\n\\n\")\n",
    "# print(\"bigbird pooler output\", pooled_output, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN THIS FOR WEIGHTS CONVERSION\n",
    "\n",
    "# from transformers import BigBirdForPreTraining, BigBirdConfig, load_tf_weights_in_big_bird\n",
    "\n",
    "# config = BigBirdConfig()\n",
    "# model = BigBirdForPreTraining(config)\n",
    "\n",
    "# old = model.state_dict()\n",
    "\n",
    "# model = load_tf_weights_in_big_bird(model, \"ckpt/bigbr_base/model.ckpt-0\")\n",
    "\n",
    "# model.save_pretrained(\"google/bigbird-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference bw input_ids: 0\n",
      "difference bw word_embeddings: 0.0\n",
      "difference bw l1 layer_input 7.1525574e-07\n",
      "difference bw l1 layer_output 5.2452087e-06\n",
      "difference bw last layer_output 4.5657158e-05\n",
      "difference bw bigbird sequence out 4.5657158e-05\n",
      "\n",
      "difference bw bigbird pooler output 1.5199184e-06\n",
      "\n",
      "difference bw bigbird masked_lm_log_probs 0.00032615662\n",
      "\n",
      "difference bw bigbird next_sentence_log_probs 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"difference bw input_ids:\", difference_between_tensors(model.input_ids, hf_model.bert.input_ids))\n",
    "print(\"difference bw word_embeddings:\", difference_between_tensors(model.word_embeddings, hf_model.bert.word_embeddings))\n",
    "\n",
    "print(\"difference bw l1 layer_input\", difference_between_tensors(model.encoder.l1_layer_input, hf_model.bert.encoder.l1_layer_input))\n",
    "\n",
    "print(\"difference bw l1 layer_output\", difference_between_tensors(model.encoder.l1_layer_output, hf_model.bert.encoder.l1_layer_output))\n",
    "print(\"difference bw last layer_output\", difference_between_tensors(model.encoder.last_layer_output,hf_model.bert.encoder.last_layer_output))\n",
    "\n",
    "print(\"difference bw bigbird sequence out\", difference_between_tensors(sequence_output, hf_sequence_output), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw bigbird pooler output\", difference_between_tensors(pooler_output, hf_pooler_output), end=\"\\n\\n\")\n",
    "\n",
    "print(\"difference bw bigbird masked_lm_log_probs\", difference_between_tensors(masked_lm_log_probs, hf_masked_lm_log_probs), end=\"\\n\\n\")\n",
    "print(\"difference bw bigbird next_sentence_log_probs\", difference_between_tensors(next_sentence_log_probs, hf_next_sentence_log_probs), end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.train.list_variables(\"ckpt/bigbr_base/model.ckpt-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.big_bird.modeling_big_bird import BigBirdAttention\n",
    "# from bigbird.core.attention import MultiHeadedAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # layer-0 debugging\n",
    "\n",
    "# print(\"difference bw k:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.k, hf_model.bert.encoder.layer[0].attention.self.k\n",
    "# ))\n",
    "\n",
    "# print(\"difference bw q:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.q, hf_model.bert.encoder.layer[0].attention.self.q\n",
    "# ))\n",
    "\n",
    "# print(\"difference bw v:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.v, hf_model.bert.encoder.layer[0].attention.self.v\n",
    "# ))\n",
    "\n",
    "# print(\"difference bw v:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.v, hf_model.bert.encoder.layer[0].attention.self.v\n",
    "# ))\n",
    "\n",
    "# print(\"difference bw attn_sc:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.attn_sc, hf_model.bert.encoder.layer[0].attention.self.attn_sc\n",
    "# ))\n",
    "\n",
    "# print(\"difference bw attn_p:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.attn_p, hf_model.bert.encoder.layer[0].attention.self.attn_p\n",
    "# ))\n",
    "\n",
    "\n",
    "# print(\"difference bw attn_o:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.attn_o, hf_model.bert.encoder.layer[0].attention.self.attn_o\n",
    "# ))\n",
    "\n",
    "# print(\"difference bw attn_proj_o:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_proj_o, hf_model.bert.encoder.layer[0].attn_proj_o\n",
    "# ))\n",
    "# \n",
    "# print(\"difference bw do:\", difference_between_tensors(model.encoder.encoder_layers[0].do, hf_model.bert.encoder.layer[0].output.do\n",
    "# ))\n",
    "\n",
    "# print(\"difference bw l_o:\", difference_between_tensors(model.encoder.encoder_layers[0].l_o, hf_model.bert.encoder.layer[0].l_o\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"difference bw hs:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.hs, hf_model.bert.encoder.layer[0].attention.self.hs))\n",
    "# print(\"difference bw bm:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.bm, hf_model.bert.encoder.layer[0].attention.self.bm))                                                     \n",
    "# print(\"difference bw fm:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.fm, hf_model.bert.encoder.layer[0].attention.self.fm))\n",
    "# print(\"difference bw tm:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.tm, hf_model.bert.encoder.layer[0].attention.self.tm))\n",
    "# print(\"difference bw fbm:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.fbm, hf_model.bert.encoder.layer[0].attention.self.fbm))\n",
    "# print(\"difference bw tbm:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.tbm, hf_model.bert.encoder.layer[0].attention.self.tbm))\n",
    "\n",
    "# print(\"difference bw ran:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.ran, hf_model.bert.encoder.layer[0].attention.self.ran))\n",
    "\n",
    "# print(\"difference bw rand_mask:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.rand_mask, hf_model.bert.encoder.layer[0].attention.self.rand_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"difference bw gk:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.gk, hf_model.bert.encoder.layer[0].attention.self.gk))\n",
    "# print(\"difference bw gv:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.gv, hf_model.bert.encoder.layer[0].attention.self.gv))\n",
    "\n",
    "# print(\"difference bw fcl:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.fcl, hf_model.bert.encoder.layer[0].attention.self.fcl))\n",
    "\n",
    "# print(\"difference bw fcl:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.scl, hf_model.bert.encoder.layer[0].attention.self.scl))\n",
    "# print(\"difference bw cl:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.cl, hf_model.bert.encoder.layer[0].attention.self.cl))\n",
    "\n",
    "# print(\"difference bw slcl:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.cl, hf_model.bert.encoder.layer[0].attention.self.slcl))\n",
    "\n",
    "\n",
    "# print(\"difference bw final_cl:\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.final_cl, hf_model.bert.encoder.layer[0].attention.self.final_cl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacement of tf.gather in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def torch_gather_b2(params, indices):\n",
    "#     batch_dims = 2\n",
    "#     assert params.shape[:batch_dims] == indices.shape[:batch_dims]\n",
    "#     out_shape = indices.shape + params.shape[-1:]\n",
    "\n",
    "#     out = torch.stack(\n",
    "#         [torch.stack(\n",
    "#             [p2[i2.flatten()] for p2, i2 in zip(p1, i1)]\n",
    "#         ) for p1, i1 in zip(params, indices)]\n",
    "#     )\n",
    "#     return out.view(out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# params = np.random.randn(2, 12, 256, 16, 3)\n",
    "# indices = np.random.randint(2, dtype=np.int32, size=(2, 12, 256, 3))\n",
    "\n",
    "# tf_p = tf.convert_to_tensor(params)\n",
    "# tf_i = tf.convert_to_tensor(indices)\n",
    "\n",
    "# py_p = torch.from_numpy(params)\n",
    "# py_i = torch.from_numpy(indices).long()\n",
    "\n",
    "# # output.shape = params.shape[:axis] + indices.shape[batch_dims:] + params.shape[axis + 1:]\n",
    "\n",
    "# out_tf = tf.gather(tf_p, tf_i, batch_dims=3)\n",
    "# out_pt = torch_gather_b3(py_p, py_i).view((2,12,256,3,3))\n",
    "# # out_tf = tf.gather(tf_p, tf_i, batch_dims=1)\n",
    "# # params = py_p\n",
    "# # indices = py_i\n",
    "# # out_pt = torch.stack([p1[i1.flatten()] for p1, i1 in zip(params, indices)]).view(indices.shape + params.shape[-2:])\n",
    "# np.max(np.abs(out_pt.numpy() - out_tf.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 6, 128, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "hf_model.bert.encoder.layer[0].attention.self.gk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 6, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "hf_model.bert.encoder.layer[0].attention.self.ran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [4],\n",
       "        [7],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1]])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "hf_model.bert.encoder.layer[0].attention.self.ran[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 128\n",
    "block_size = 16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('transformers': conda)",
   "metadata": {
    "interpreter": {
     "hash": "43b8762cdba22b1f3661f53828ffc27c829b0d988d5ae49721f2db103a874ee7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}